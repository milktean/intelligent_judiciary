**Task:**

Evaluate the performance of a legal language model in a judicial consultation dialogue. Carefully read the conversation between the user and the assistant, and assess the assistant's response specifically in the **infer** section based on the criteria of accuracy, completeness, relevance, and effectiveness.

**Instructions:**

- Assign a score from 0 to 5 for each criterion using the provided scoring guidelines.
- Provide specific explanations for each score, using professional legal terminology and analytical language.
- **Output only the specified JSON format, do not add any other text or explanation.**

**Scoring Criteria:**

- **Accuracy (0-5):**
  - **5:** All legal documents, regulations, and terms mentioned are accurate.
  - **4:** ≥80% are accurate.
  - **3:** ≥60% and <80% are accurate.
  - **2:** ≥40% and <60% are accurate.
  - **1:** ≥20% and <40% are accurate.
  - **0:** Mostly incorrect or fabricated.

- **Completeness (0-5):**
  - **5:** Answered all types of legal questions posed by the user.
  - **4:** Answered ≥80% but not all types.
  - **3:** Answered ≥60% and <80% types.
  - **2:** Answered ≥40% and <60% types.
  - **1:** Answered ≥20% and <40% types.
  - **0:** Answered almost none.

- **Relevance (0-5):**
  - **5:** The case details and recommended similar cases fully reflect the user's query.
  - **4:** Reflects ≥80% but not all relevance.
  - **3:** Reflects ≥60% and <80% relevance.
  - **2:** Reflects ≥40% and <60% relevance.
  - **1:** Reflects ≥20% and <40% relevance.
  - **0:** Barely reflects relevance.

- **Effectiveness (0-5):**
  - **5:** Suggestions help resolve case-related issues, answer questions, and are actionable.
  - **4:** Helps resolve issues but answers are unclear; actionability is moderate.
  - **3:** Indirectly helps; answers are unclear; somewhat actionable.
  - **2:** Indirectly helps with few issues; answers are unclear; limited actionability.
  - **1:** Indirectly helps with few issues; answers are unclear; not actionable.
  - **0:** Does not help; does not answer the questions.

**Output Format:**

**Output only the following JSON format, do not add any other text or explanation.**

```json
{
  "model_id": "",  // Identifier of the evaluated model
  "accuracy": {
    "score": 0,  // Score (0-5)
    "explanation": ""  // Explanation for the score
  },
  "completeness": {
    "score": 0,
    "explanation": ""
  },
  "relevance": {
    "score": 0,
    "explanation": ""
  },
  "effectiveness": {
    "score": 0,
    "explanation": ""
  },
  "average_score": 0  // Average of the four scores
}
```

**Content:**

Evaluate the assistant's response (**infer**) in the following conversation:

- **Conversation History Placeholder:**

  ```
  {{conversation_history}}
  ```

- **Model's Output Placeholder (Needs Evaluation):**

  ```
  {{model_output}}
  ```

